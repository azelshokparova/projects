# -*- coding: utf-8 -*-
"""
Automatically generated by Colab.


"""

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from tqdm.notebook import tqdm

##dataset with female names

!wget http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/female.txt

device = 'cuda'

#read lines
with open('female.txt', 'r') as f:
    lines = f.readlines() #all names in lines are imported

names = []
max_len = 0
for l in lines[6:]:
  curr_name = l[:-1].lower() #change names into lower capitals # -1 to erase the line spacing
  if curr_name.isalpha():
    names.append(curr_name)
    max_len = max(len(names[-1]), max_len)
max_len += 1 # consider the 'EOS' (end of signal)
print('Maximum Length : ' + str(max_len))

class NameDataset(Dataset):
    def __init__(self, names, max_len):
        self.names = names
        self.max_len = max_len
        self.a_order = ord('a')
        self.z_order = ord('z')
        self.num_classes = 26 + 1 # a-z + include the end of signal information <EOS>

    def __len__(self):
        return len(self.names)

    def __getitem__(self, idx):
        padding_name = [self.num_classes-1 for _ in range(self.max_len)] #list
        curr_name = [ord(n)-self.a_order for n in names[idx]] #list with the indexes of letters of the name
        padding_name[:len(curr_name)] = curr_name

        # Slide the input to make a output
        sample = dict()
        # -1 is end of signal
        #maybe the length is according to the max_len
        sample['input'] = torch.LongTensor(padding_name[:-1]) # h y e m i n  -1 -1 -1
        sample['output'] = torch.LongTensor(padding_name[1:]) # y e m i n -1 -1 -1 -1
        sample['length'] = len(names[idx])
        sample['original'] = names[idx]

        return sample

batch_size = 64
dataset = NameDataset(names, max_len)
dataloader = DataLoader(dataset, batch_size=64, shuffle=True)

sample = next(iter(dataloader))
print(sample['input'][0])
print(sample['output'][0])
print(sample['length'][0])
print(sample['original'][0])
print(sample['input'].shape, sample['output'].shape)

# This could be useful with variable lengths
#not really needed here
total_lengths = sample['length']
sort_length, sort_idx = torch.sort(total_lengths, descending=True)
sort_input = sample['input'][sort_idx]
sort_output = sample['output'][sort_idx]
print(sort_length)
print(sort_input.shape)

class RNNmodel(nn.Module):
    def __init__(self, lstm_dim=256, num_classes=dataset.num_classes, max_len=max_len):
        super(RNNmodel, self).__init__()
        self.lstm_dim = lstm_dim
        self.num_classes = num_classes
        self.max_len = max_len
        self.char_embedding = nn.Embedding(num_embeddings=num_classes,
                                           embedding_dim=lstm_dim)
        self.lstm = nn.LSTM(input_size=lstm_dim,
                            hidden_size=lstm_dim,
                            num_layers=1,
                            batch_first=True,
                            )

        self.out_linear = nn.Linear(lstm_dim, num_classes)

    def forward(self, sort_input, sort_output, sort_length):
        ## originally, recommended to use torch.nn.utils.rnn.pack_padded_sequence,when we have variable lengths
        ## but in this case, I just neglected it because beginners can be more confused with this
        lstm_input = self.char_embedding(sort_input)
        lstm_out, (h, c) = self.lstm(lstm_input)
        out = self.out_linear(lstm_out)

        return out

    def test(self, start_char):
        generated_name = list()
        generated_name.append(start_char)

        start_order = torch.LongTensor([ord(start_char)]).to(device) - ord('a')
        start_order = start_order.reshape(1, 1)
        cnt = 0

        while cnt <= self.max_len:
            curr_embed = self.char_embedding(start_order)
            if cnt == 0:
                lstm_out, (h, c) = self.lstm(curr_embed)
            else:
                lstm_out, (h, c) = self.lstm(curr_embed, (h, c))
            out = self.out_linear(lstm_out)

            sample_next = torch.distributions.Categorical(logits = out[0, 0, :]).sample().item()
            if sample_next == 26:
                break
            else:
                generated_name.append(chr(ord('a')+sample_next))
                sample_next = torch.LongTensor([sample_next]).to(device)
                start_order = sample_next.reshape(1, 1)

                cnt += 1

        return ''.join(generated_name)

model = RNNmodel()
model = model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

def train(model, optimizer, sample):
    optimizer.zero_grad()
    criteria = nn.CrossEntropyLoss()

    total_lengths = sample['length']
    sort_length, sort_idx = torch.sort(total_lengths, descending=True)

    sort_input = sample['input'][sort_idx].to(device)
    sort_output = sample['output'][sort_idx].to(device)
    sort_length = sort_length.to(device)

    pred = model(sort_input, sort_output, sort_length) # B T C
    B, T, C = pred.shape

    curr_loss = criteria(pred.reshape(B*T, C), sort_output.reshape(B*T))

    curr_loss.backward()
    optimizer.step()

    return curr_loss.item()

max_epoch = 200
for epoch in tqdm(range(max_epoch)):
    total_loss = 0.0
    for sample in dataloader:
        curr_loss = train(model, optimizer, sample)
        total_loss += curr_loss / len(dataloader)

    start_char = chr(np.random.randint(ord('a'), ord('z')))
    print('[EPOCH {}] TRAIN LOSS: {}, SAMPLED NAME: {}'.format(epoch,
                                                               total_loss,
                                                               model.test(start_char)))