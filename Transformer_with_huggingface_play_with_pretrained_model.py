# -*- coding: utf-8 -*-
"""
Automatically generated by Colab.

## Install hugging face transformer.
"""

!pip install transformers dataset

"""## Define various pretrained model for various tasks"""

from transformers import pipeline

classifier = pipeline(task="sentiment-analysis", model="cardiffnlp/twitter-roberta-base-sentiment-latest")
generator = pipeline(task='text-generation', model="distilgpt2")
fill_mask = pipeline(task='fill-mask', model='bert-base-uncased') ##fills the blank gaps
question_answering = pipeline(task='question-answering', model="deepset/roberta-base-squad2", tokenizer='deepset/roberta-base-squad2')

"""## Sentiment Classifier"""

classifier("It is too cold outside. I don't want to go outside today.")

results = classifier(["It is too cold outside. I don't want to go outside today.", "But Christmas is coming!"])
print(results)
for res in results:
    print("label: {}, with score: {}".format(res['label'], res['score']))

"""## Text Generator"""

results = generator(['Baby it is cold outside', 'Finally it is summer time'])

print(results[0][0]['generated_text'])
print('*'*50)
print(results[1][0]['generated_text'])

"""## Fill Mask"""

fill_results = fill_mask('A man is in a kitchen, and he is holding an empty mug. He walks towards a coffee machine to get [MASK].') #prompt engineering

print(fill_results)

for res in fill_results:
  print(res['token_str'])

fill_results[0] #dictionary

"""## Question and Answering"""

QA_input = {'question': 'What should a robot bring to man?',
            'context': fill_results[0]['sequence']}
results = question_answering(QA_input)
print(results)

"""## Sentence Similarity based on the sentence bert"""

model_name = "sentence-transformers/all-MiniLM-L6-v2"

from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F

model = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

feature_extractor = pipeline('feature-extraction', model=model, tokenizer=tokenizer) # one method is to use the pipeline

"""## The first method - non parallel computation"""

sentences = ['In this paper, we propose a novel method called Transformer.', 'We propose a novel model named Transformer.']

#torch.Tensor(feature_extractor(sentences[0])).shape #15 tokens of dimension 384

feature_1 = torch.Tensor(feature_extractor(sentences[0]))
feature_2 = torch.Tensor(feature_extractor(sentences[1]))

#by taking mean, the shape will be 1 by 384 > and we want normalizing the dimanio 384 which is the first from the end
norm_feat_1 = F.normalize(torch.mean(feature_1, dim=1), dim=-1)
norm_feat_2 = F.normalize(torch.mean(feature_2, dim=1), dim=-1)
similarity = torch.sum( norm_feat_1 * norm_feat_2 )
print('TWO SENTENCE Similarity: ', similarity.item())

"""## The second method - parallel computation with padding (recommended)"""

sentences = ['In this paper, we propose a novel method called Transformer.', 'We propose a novel model named Transformer.']

#because the sentences are of different length, we need to add some paddings to make the length the same
#input ids are the ids of tokens in the vocabulary
encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt') #for parallel computation we need to do padding, token is a somehting that has a meaning
print(encoded_input)
print(encoded_input['attention_mask'].shape)

with torch.no_grad():
    model_output = model(**encoded_input)
print(model_output.keys())
print(model_output['last_hidden_state'].shape)
print(model_output['last_hidden_state'][-1,:,0])

#Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)

similarity = (sentence_embeddings[0] * sentence_embeddings[1]).sum()
print('TWO SENTENCE SIMILARITY: ', similarity.item())